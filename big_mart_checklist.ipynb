{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0727d55f-3007-45ce-a974-70e358b8e9a6",
   "metadata": {},
   "source": [
    "# Data Modeling in Machine Learning (Regression)\n",
    "\n",
    "This notebook is part of our tutorial series on **data modeling in machine learning**. It aims to act as a subscript to our tutorial video on the subject (data modeling).\n",
    "\n",
    "In this notebook, we explore the [BigMart_Sales_Prediction](data/big_mart_sales_prediction.csv) dataset. This is a regression dataset consisting of twelve (12) feature columns described below: \n",
    "\n",
    "- **Item_Identifier**: Unique product ID\n",
    "- **Item_Weight**: Weight of the product\n",
    "- **Item_Fat_Content**: Whether the product is low fat or not\n",
    "- **Item_Visibility**: The % of the total display area of all products in a store allocated to the particular product\n",
    "- **Item_Type**: The category to which the product belongs\n",
    "- **Item_MRP**: Maximum Retail Price (list price) of the product\n",
    "- **Outlet_Identifier**: Unique store ID\n",
    "- **Outlet_Establishment_Year**: The year in which the store was established\n",
    "- **Outlet_Size**: The size of the store in terms of ground area covered\n",
    "- **Outlet_Location_Type**: The type of city in which the store is located\n",
    "- **Outlet_Type**: Whether the outlet is just a grocery store or some sort of supermarket\n",
    "- **Item_Outlet_Sales**: Sales of the product in the particular store. This is the outcome variable to be predicted.\n",
    "\n",
    "What we hope to achieve here is to examine the data modeling process on the [BigMart_Sales_Prediction](data/big_mart_sales_prediction.csv) dataset making use of our data modeling [checklist](data_modelling_checklist.md) described [below](#What-are-the-steps-(checklist)-in-modeling-data?). Before going too far let's understand some concepts before we begin with this tutorial.\n",
    "\n",
    "- What is data modeling in machine learning?\n",
    "- Why do we model data in machine learning?\n",
    "- What are regression tasks?\n",
    "- What are the steps (checklist) in modeling data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d156742-23e8-4eb9-a6c8-2f43a598221d",
   "metadata": {},
   "source": [
    "## What is data modeling in machine learning?\n",
    "\n",
    "**Data modeling in machine learning**¬†is the process of **creating machine learning models capable of** predicting labels from features, tuning them for the business need, and validating it on holdout data. The output from data modeling is a trained model that can be used for **making predictions on new data points**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aab869-48d2-41c5-ae85-3e0daaa03e46",
   "metadata": {},
   "source": [
    "## Why do we model data in machine learning?\n",
    "\n",
    "We all know that the purpose of machine learning is to create models that mimic understanding of real-world situations through data and apply their understanding to real-world scenarios. We model data to create a bridge (pipeline) between the dataset and the machine learning model, such that new data can be quickly received and translated to results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5a172-542e-48d7-8882-235ca1a2b1c2",
   "metadata": {},
   "source": [
    "## What are regression tasks?\n",
    "\n",
    "Regression tasks are continuous. They require mathematical methods that allow data scientists to predict a continuous outcome (y) based on the value of one or more predictor variables (x). Where x is the input value/signal and y is the output value/result.\n",
    "\n",
    "Our [BigMart_Sales_Prediction](data/big_mart_sales_prediction.csv) dataset is a regression task and what we aim to predict is the `Item_Outlet_Sales` (y)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bc494f-94ca-4939-9cb8-2d6272cbd1da",
   "metadata": {},
   "source": [
    "## What are the steps (checklist) in modeling data?\n",
    "\n",
    "There are four major steps involved in data modeling tasks. These are **data preparation**, **shortlisting promising models**, **fine-tuning the selected systems**, and **finally presenting your solution**.\n",
    "\n",
    "The full checklist of a data modeling process can be found [here](ml_project_checklist.docx). The checklist was written by [Aur√©lien G√©ron](https://www.oreilly.com/people/aurelien-geron/) in his book [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/). But the checklist we'll be making use of is more fine-tuned for data modeling alone. \n",
    "\n",
    "**üìñ** I strongly recommend this book [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) to everyone new to the field of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a20d62-357f-43f4-8319-d374de051c19",
   "metadata": {
    "tags": [
     "data_cleaning"
    ]
   },
   "source": [
    "### Prepare the Data\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- Work on copies of the data (keep the original dataset intact).\n",
    "- Write functions for all data transformations you apply, for five reasons:\n",
    "    - So you can easily prepare the data the next time you get a fresh dataset\n",
    "    - So you can apply these transformations in future projects\n",
    "    - To clean and prepare the test set\n",
    "    - To clean and prepare new data instances once your solution is live\n",
    "    - To make it easy to treat your preparation choices as hyperparameters\n",
    "\n",
    "1. **Data cleaning**:\n",
    "    - Fix or remove outliers (optional).\n",
    "    - Fill in missing values (e.g., with zero, mean, median‚Ä¶) or drop their rows (or columns).\n",
    "    \n",
    "2. **Feature selection (optional)**:\n",
    "    - Drop the attributes that provide no useful information for the task.\n",
    "    \n",
    "3. **Feature engineering, where appropriate**:\n",
    "    - Discretize continuous features.\n",
    "    - Decompose features (e.g., categorical, date/time, etc.).\n",
    "    - Add promising transformations of features (e.g., log(x), sqrt(x), x2, etc.).\n",
    "    - Aggregate features into promising new features.\n",
    "\n",
    "4. **Feature scaling**:\n",
    "    - Standardize or normalize features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185a2978-cec7-4310-8071-fc5272b3fb57",
   "metadata": {},
   "source": [
    "### Shortlist Promising Models\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- If the data is huge, you may want to sample smaller training sets so you can train many different models in a reasonable time (be aware that this penalizes complex models such as large neural nets or Random Forests).\n",
    "- Once again, try to automate these steps as much as possible.\n",
    "\n",
    "1. Train many quick-and-dirty models from different categories (e.g., linear, naive Bayes, SVM, Random Forest, neural net, etc.) using standard parameters.\n",
    "2. Measure and compare their performance.\n",
    "    - For each model, use N -fold cross-validation and compute the mean and standard deviation of the performance measure on the N folds.\n",
    "3. Analyze the most significant variables for each algorithm.\n",
    "4. Analyze the types of errors the models make.\n",
    "    - What data would a human have used to avoid these errors?\n",
    "5. Perform a quick round of feature selection and engineering.\n",
    "6. Perform one or two more quick iterations of the five previous steps.\n",
    "7. Shortlist the top three to five most promising models, preferring models that make different types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3734c6-0956-47a3-87ec-266fd58c74c9",
   "metadata": {},
   "source": [
    "### Fine-Tune the System\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- You will want to use as much data as possible for this step, especially as you move toward the end of fine-tuning.\n",
    "- As always, automate what you can.\n",
    "\n",
    "1. Fine-tune the hyperparameters using cross-validation:\n",
    "    - Treat your data transformation choices as hyperparameters, especially when you are not sure about them (e.g., if you‚Äôre not sure whether to replace missing values with zeros or with the median value, or to just drop the rows).\n",
    "    - Unless there are very few hyperparameter values to explore, prefer random search over grid search. If training is very long, you may prefer a Bayesian optimization approach (e.g., using Gaussian process priors, as described by Jasper Snoek et al. ).\n",
    "2. Try Ensemble methods. Combining your best models will often produce better performance than running them individually.\n",
    "3. Once you are confident about your final model, measure its performance on the test set to estimate the generalization error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92384646-1385-490f-adfa-95a9f76059c1",
   "metadata": {},
   "source": [
    "### Present Your Solution\n",
    "\n",
    "1. Document what you have done.\n",
    "2. Create a nice presentation.\n",
    "    - Make sure you highlight the big picture first.\n",
    "3. Explain why your solution achieves the business objective.\n",
    "4. Don‚Äôt forget to present interesting points you noticed along the way.\n",
    "    - Describe what worked and what did not.\n",
    "    - List your assumptions and your system‚Äôs limitations.\n",
    "5. Ensure your key findings are communicated through beautiful visualizations or easy-to-remember statements (e.g., ‚Äúthe median income is the number-one predictor of housing prices‚Äù)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba908e87-4f63-4eb4-b820-90bd2e9c2aa4",
   "metadata": {},
   "source": [
    "### Side Note\n",
    "\n",
    "There are a couple of things I want to point out before we begin. The emojis represented in black quotes stand for different things entirely.\n",
    "\n",
    "**‚Åâ** stand for research or reason finding. At any point you encounter this emoji, you're expected to answer the question and get the intuition behind it.\n",
    "\n",
    "**üõ†** stands for activities. The activities described here are all to be compiled and submitted to the class telegram page.\n",
    "\n",
    "**üìñ** stands for resources. You're expected to follow up on every resource provided in this notebook to aid you in better understanding the task. I would be joking to tell you one single notebook would help you grasp the entirety of data modeling in machine learning.\n",
    "\n",
    "Now that we've explained all of the steps, let's proceed to mark our checklist üòä."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d13300e9-5781-4005-b791-dbd6b242e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22034288-aa1f-463a-b7da-20d738517458",
   "metadata": {},
   "source": [
    "Let's import our dataset as `big_mart_sales`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6680252-fb0b-491d-b04c-1cebc9a1d1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Identifier</th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Fat_Content</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_Type</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Identifier</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Outlet_Size</th>\n",
       "      <th>Outlet_Location_Type</th>\n",
       "      <th>Outlet_Type</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FDA15</td>\n",
       "      <td>9.30</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016047</td>\n",
       "      <td>Dairy</td>\n",
       "      <td>249.8092</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>3735.1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DRC01</td>\n",
       "      <td>5.92</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.019278</td>\n",
       "      <td>Soft Drinks</td>\n",
       "      <td>48.2692</td>\n",
       "      <td>OUT018</td>\n",
       "      <td>2009</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type2</td>\n",
       "      <td>443.4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FDN15</td>\n",
       "      <td>17.50</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>Meat</td>\n",
       "      <td>141.6180</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>2097.2700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FDX07</td>\n",
       "      <td>19.20</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fruits and Vegetables</td>\n",
       "      <td>182.0950</td>\n",
       "      <td>OUT010</td>\n",
       "      <td>1998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Grocery Store</td>\n",
       "      <td>732.3800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCD19</td>\n",
       "      <td>8.93</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Household</td>\n",
       "      <td>53.8614</td>\n",
       "      <td>OUT013</td>\n",
       "      <td>1987</td>\n",
       "      <td>High</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>994.7052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Item_Identifier  Item_Weight Item_Fat_Content  Item_Visibility  \\\n",
       "0           FDA15         9.30          Low Fat         0.016047   \n",
       "1           DRC01         5.92          Regular         0.019278   \n",
       "2           FDN15        17.50          Low Fat         0.016760   \n",
       "3           FDX07        19.20          Regular         0.000000   \n",
       "4           NCD19         8.93          Low Fat         0.000000   \n",
       "\n",
       "               Item_Type  Item_MRP Outlet_Identifier  \\\n",
       "0                  Dairy  249.8092            OUT049   \n",
       "1            Soft Drinks   48.2692            OUT018   \n",
       "2                   Meat  141.6180            OUT049   \n",
       "3  Fruits and Vegetables  182.0950            OUT010   \n",
       "4              Household   53.8614            OUT013   \n",
       "\n",
       "   Outlet_Establishment_Year Outlet_Size Outlet_Location_Type  \\\n",
       "0                       1999      Medium               Tier 1   \n",
       "1                       2009      Medium               Tier 3   \n",
       "2                       1999      Medium               Tier 1   \n",
       "3                       1998         NaN               Tier 3   \n",
       "4                       1987        High               Tier 3   \n",
       "\n",
       "         Outlet_Type  Item_Outlet_Sales  \n",
       "0  Supermarket Type1          3735.1380  \n",
       "1  Supermarket Type2           443.4228  \n",
       "2  Supermarket Type1          2097.2700  \n",
       "3      Grocery Store           732.3800  \n",
       "4  Supermarket Type1           994.7052  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_mart_sales = pd.read_csv(\"data/big_mart_sales_prediction.csv\")\n",
    "big_mart_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6393214-82b3-4d55-8e53-5779f29443ca",
   "metadata": {},
   "source": [
    "We then proceed to using the `.info` method to find out more about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017a8574-172e-4822-af9c-8a5ade92b826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8523 entries, 0 to 8522\n",
      "Data columns (total 12 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Item_Identifier            8523 non-null   object \n",
      " 1   Item_Weight                7060 non-null   float64\n",
      " 2   Item_Fat_Content           8523 non-null   object \n",
      " 3   Item_Visibility            8523 non-null   float64\n",
      " 4   Item_Type                  8523 non-null   object \n",
      " 5   Item_MRP                   8523 non-null   float64\n",
      " 6   Outlet_Identifier          8523 non-null   object \n",
      " 7   Outlet_Establishment_Year  8523 non-null   int64  \n",
      " 8   Outlet_Size                6113 non-null   object \n",
      " 9   Outlet_Location_Type       8523 non-null   object \n",
      " 10  Outlet_Type                8523 non-null   object \n",
      " 11  Item_Outlet_Sales          8523 non-null   float64\n",
      "dtypes: float64(4), int64(1), object(7)\n",
      "memory usage: 799.2+ KB\n"
     ]
    }
   ],
   "source": [
    "big_mart_sales.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498cee8-7b0c-48cd-9135-428982b11e27",
   "metadata": {},
   "source": [
    "#### Prepare the data\n",
    "---\n",
    "\n",
    "From our checklist, the first thing we have to do is **prepare the data**. We start by cleaning the data by filling the missing values in the dataset.\n",
    "\n",
    "What can we tell from the `.info` method?\n",
    "\n",
    "**The first thing** we notice is the incomplete columns which are the `Item_Weight` and `Outlet_Size` which both have 1463 and 2410 missing values respectively.\n",
    "\n",
    "**The second thing** we can tell is that the feature columns can be divided into two (2) distinct set, **Item_** and **Outlet_**, which represents the items and outlets respectively.\n",
    "\n",
    "> **üõ†**: Use this difference to fill the missing column feature column `Item_Weight` and `Outlet_Size`. At the end of this notebook, create a model that automatically fills this columns. But for now, we should proceed to study this columns and decide on a simpler way of filling the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d4aa2aa-2060-45b2-89e3-c58636ab4692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     9.30\n",
       "1     5.92\n",
       "2    17.50\n",
       "3    19.20\n",
       "4     8.93\n",
       "Name: Item_Weight, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_mart_sales.Item_Weight.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9017131c-6e2d-4659-9c85-8815cf34735f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Medium', nan, 'High', 'Small'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_mart_sales.Outlet_Size.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833a8674-dbe1-4f89-9bde-6a71717d4e3e",
   "metadata": {},
   "source": [
    "The `Item_Weight` feature column is a continuous variable, while the `Outlet_Size` feature column is a categorical variable so one way we can fill these columns is by filling the `Item_Weight` feature column with the mean value from that feature and performing forward filling on the `Outlet_Size` feature column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d9701a-a11c-46e5-b32f-df12b52409fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_mart_sales.Item_Weight.fillna(big_mart_sales.Item_Weight.mean(), inplace=True)\n",
    "big_mart_sales.Outlet_Size.ffill(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d549195e-2f1e-41dc-b99c-19531600ef9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8523 entries, 0 to 8522\n",
      "Data columns (total 12 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Item_Identifier            8523 non-null   object \n",
      " 1   Item_Weight                8523 non-null   float64\n",
      " 2   Item_Fat_Content           8523 non-null   object \n",
      " 3   Item_Visibility            8523 non-null   float64\n",
      " 4   Item_Type                  8523 non-null   object \n",
      " 5   Item_MRP                   8523 non-null   float64\n",
      " 6   Outlet_Identifier          8523 non-null   object \n",
      " 7   Outlet_Establishment_Year  8523 non-null   int64  \n",
      " 8   Outlet_Size                8523 non-null   object \n",
      " 9   Outlet_Location_Type       8523 non-null   object \n",
      " 10  Outlet_Type                8523 non-null   object \n",
      " 11  Item_Outlet_Sales          8523 non-null   float64\n",
      "dtypes: float64(4), int64(1), object(7)\n",
      "memory usage: 799.2+ KB\n"
     ]
    }
   ],
   "source": [
    "big_mart_sales.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55584693-d9e0-4d98-aeb8-abe1618b7f0b",
   "metadata": {},
   "source": [
    "We've successfully cleaned the dataset ‚úî\n",
    "\n",
    "#### Feature Selection\n",
    "---\n",
    "\n",
    "The next thing on our checklist is **feature selection**, which deals with excluding attributes (feature columns) that provide little to no useful information for the task. We'll be skipping this because every feature column is properly useful to our task which is predicting the `Item_Outlet_Sales`.\n",
    "\n",
    "After that would be feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bae6c0f-eca5-4ffa-ab5b-51436717e959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8523.000000</td>\n",
       "      <td>8523.000000</td>\n",
       "      <td>8523.000000</td>\n",
       "      <td>8523.000000</td>\n",
       "      <td>8523.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.857645</td>\n",
       "      <td>0.066132</td>\n",
       "      <td>140.992782</td>\n",
       "      <td>1997.831867</td>\n",
       "      <td>2181.288914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.226124</td>\n",
       "      <td>0.051598</td>\n",
       "      <td>62.275067</td>\n",
       "      <td>8.371760</td>\n",
       "      <td>1706.499616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.555000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.290000</td>\n",
       "      <td>1985.000000</td>\n",
       "      <td>33.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.310000</td>\n",
       "      <td>0.026989</td>\n",
       "      <td>93.826500</td>\n",
       "      <td>1987.000000</td>\n",
       "      <td>834.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.857645</td>\n",
       "      <td>0.053931</td>\n",
       "      <td>143.012800</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>1794.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.094585</td>\n",
       "      <td>185.643700</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>3101.296400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.350000</td>\n",
       "      <td>0.328391</td>\n",
       "      <td>266.888400</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>13086.964800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Item_Weight  Item_Visibility     Item_MRP  Outlet_Establishment_Year  \\\n",
       "count  8523.000000      8523.000000  8523.000000                8523.000000   \n",
       "mean     12.857645         0.066132   140.992782                1997.831867   \n",
       "std       4.226124         0.051598    62.275067                   8.371760   \n",
       "min       4.555000         0.000000    31.290000                1985.000000   \n",
       "25%       9.310000         0.026989    93.826500                1987.000000   \n",
       "50%      12.857645         0.053931   143.012800                1999.000000   \n",
       "75%      16.000000         0.094585   185.643700                2004.000000   \n",
       "max      21.350000         0.328391   266.888400                2009.000000   \n",
       "\n",
       "       Item_Outlet_Sales  \n",
       "count        8523.000000  \n",
       "mean         2181.288914  \n",
       "std          1706.499616  \n",
       "min            33.290000  \n",
       "25%           834.247400  \n",
       "50%          1794.331000  \n",
       "75%          3101.296400  \n",
       "max         13086.964800  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_mart_sales.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e648156a-952f-4458-8f75-8db697d03038",
   "metadata": {},
   "source": [
    "We'll therefore skip both the **feature selection**. ‚úî\n",
    "\n",
    "#### Feature Engineering\n",
    "---\n",
    "\n",
    "Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data. Some of the means of feature engineering as listed from our checklist are:\n",
    "\n",
    "- Discretize continuous features.\n",
    "- Decompose features (e.g., categorical, date/time, etc.).\n",
    "- Add promising transformations of features (e.g., log(x), sqrt(x), x2, etc.).\n",
    "- Aggregate features into promising new features.\n",
    "\n",
    "There is no need to discretize continuous feature columns because none of the feature columns are best suited for discretization, there is no feature column we can decompose, there are no transformations we can apply to improve the data and there is no need to aggregate the features into a new feature.\n",
    "\n",
    "We could however attempt to decompose the `Outlet_Establishment_Year` by simplifying the year column to a much smaller value for the machine learning model. It wouldn't make much difference though so we can leave it just the way it is. \n",
    "\n",
    "> **üìñ** Feature engineering is a very important topic as it helps in reducing the dimension of the dataset while improving the chances of the dataset being accurate when applied to a model. I recommend this article [Fundamental Techniques of Feature Engineering for Machine Learning](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114) by [Emre Ren√ßberoƒülu](https://medium.com/@emrerencberoglu) for better enlightenment on the various feature engineering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde330fc-9631-4d00-9bd0-2df132c13e45",
   "metadata": {},
   "source": [
    "We however have to engineer the categorical feature. The goal is to convert all categorical columns to numbers.\n",
    "\n",
    "Scikit-Learn has provided a couple of algorithms for us to achieve this. These algorithms include the label, ordinal, and one hot encoder. Let's start by defining the various encoders and how they function.\n",
    "\n",
    "##### **Label Encoders**\n",
    "\n",
    "Label Encoding is an encoding technique for handling categorical variables. In this technique, each label is assigned a unique integer based on alphabetical ordering. \n",
    "\n",
    "![Label Encoding](images/label_encoding.png)\n",
    "\n",
    "Label encoders do not take categorical order into consideration, and this is the reason why we have ordinal encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a15d031-152c-4eae-9803-97e3f5fb6bd8",
   "metadata": {},
   "source": [
    "##### **Ordinal Encoders**\n",
    "\n",
    "Ordinal Encoding like label encoding is an encoding technique for handling categorical variables. In this technique, each unique category value is assigned an integer value. For example, red is 1, green is 2, and blue is 3.\n",
    "\n",
    "![Ordinal Encoding](images/ordinal_encoding.png)\n",
    "\n",
    "Ordinal encoders take categorical order into consideration which is good for categorical variables that has order like Big, Medium, Small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9943bb7e-55ef-4510-9e47-fc296071de88",
   "metadata": {},
   "source": [
    "##### **One-Hot Encoders**\n",
    "\n",
    "One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction. \n",
    "\n",
    "![One Hot Encoding](images/ohe.jpg)\n",
    "\n",
    "This form converts a categorical feature column to a series of columns stating if the category is present (1) or absent (0) in the new set of feature columns. Label encoders do not take categorical order into consideration, and this is the reason why we have ordinal encoders.\n",
    "\n",
    "There are 7 categorical columns in our dataset. These columns are `Item_Identifier`, `Item_Fat_Content`, `Item_Type`, `Outlet_Identifier`, `Outlet_Size`, `Outlet_Location_Type` and `Outlet_Type`. We'll have to look through them to decide on what encoder best suits a feature column.\n",
    "\n",
    "> <h3 style='display: inline;'>üÜï</h3>, üõ†: Just for fun, create function that replicate the encoders. (label, ordinal and one hot encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e0c6b20-6f84-4c51-b966-9b6310285781",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'Item_Identifier', 'Item_Fat_Content', 'Item_Type', \n",
    "    'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', \n",
    "    'Outlet_Type'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9b921b-cc9c-42ca-a256-34e888808603",
   "metadata": {},
   "source": [
    "One of the best ways to decide on the kind of encoder to make use of is by knowing the number of unique categories belonging to a feature column.\n",
    "\n",
    "The **rule of thumb** I make use of is making use of one-hot encoding for categories of 2 & 3 value, ordinal encoding for categories between 2-7 values with a certain order between them, and label encoding for every other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8285e353-7c67-43bf-b30f-58dba4ec61a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item_Identifier | ['FDA15' 'DRC01' 'FDN15' 'FDX07' 'NCD19' 'FDP36' 'FDO10' 'FDP10' 'FDH17'\n",
      " 'FDU28'] | 1559\n",
      "\n",
      "Item_Fat_Content | ['Low Fat' 'Regular' 'low fat' 'LF' 'reg'] | 5\n",
      "\n",
      "Item_Type | ['Dairy' 'Soft Drinks' 'Meat' 'Fruits and Vegetables' 'Household'\n",
      " 'Baking Goods' 'Snack Foods' 'Frozen Foods' 'Breakfast'\n",
      " 'Health and Hygiene'] | 16\n",
      "\n",
      "Outlet_Identifier | ['OUT049' 'OUT018' 'OUT010' 'OUT013' 'OUT027' 'OUT045' 'OUT017' 'OUT046'\n",
      " 'OUT035' 'OUT019'] | 10\n",
      "\n",
      "Outlet_Size | ['Medium' 'High' 'Small'] | 3\n",
      "\n",
      "Outlet_Location_Type | ['Tier 1' 'Tier 3' 'Tier 2'] | 3\n",
      "\n",
      "Outlet_Type | ['Supermarket Type1' 'Supermarket Type2' 'Grocery Store'\n",
      " 'Supermarket Type3'] | 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in categorical_columns:\n",
    "    values = big_mart_sales[col].unique()\n",
    "    \n",
    "    print(\n",
    "        col, \n",
    "        values if len(values) in range(10) else values[:10], \n",
    "        len(values), \n",
    "        sep=\" | \", end=\"\\n\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a8450-59d5-48de-884d-15b3ce490bce",
   "metadata": {},
   "source": [
    "> **Special note**: the Item_Fat_Content feature column is said to have 5 unique categories, whereas there are two (2) Low Fat and Regular. We need to adjust this column to fix that error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b8510f1-ec65-48cf-a42c-3fa01fc524aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_mart_sales.Item_Fat_Content.replace({\n",
    "    'low fat': 'Low Fat',\n",
    "    'LF': 'Low Fat',\n",
    "    'reg': 'Regular'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810c1b93-d971-49e9-af14-caf52359dd7b",
   "metadata": {},
   "source": [
    "Following my **rule of thumb**, we would apply label encoding for the categorical variable `Item_Identifier` and `Item_Type`.\n",
    "\n",
    "But for the feature columns `Item_Fat_Content`, `Outlet_Identifier`, `Outlet_Size`, `Outlet_Location_Type`, and `Outlet_Type` we would do something different. Kind of like encoding but in our own fashion.\n",
    "\n",
    "> It is good to always find a simple way to process your data if you can afford it.\n",
    "\n",
    "Let's start with our own form of encoding for the `Item_Fat_Content`, `Outlet_Identifier`, `Outlet_Size`, `Outlet_Location_Type`, and `Outlet_Type` feature columns.\n",
    "\n",
    "- The `Item_Fat_Content` feature column can follow the order 0 for Low Fat and 1 for Regular.\n",
    "- For the `Outlet_Identifier` feature column, we can simply remove the prefix 'OUT0' leaving the numbers to act as the category. We can further work on ordering the numbers to appear in the range of 0 to 9.\n",
    "- The `Outlet_Size` feature column can follow the order 0 for Small, 1 for Medium, and 2 for High.\n",
    "- For the `Outlet_Location_Type` feature column, we can simply remove the prefix 'Tier ' leaving the numbers to act as the category.\n",
    "- For the `Outlet_Type` feature column, we can simply remove the prefix 'Supermarket Type' leaving the numbers to act as the category, and replace Grocery Store with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc403fe2-f27d-4c41-84a3-cbbb4e4ef76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the Item_Fat_Content and Outlet_Size feature columns\n",
    "big_mart_sales.Item_Fat_Content.replace({'Low Fat': 0, 'Regular': 1}, inplace=True)\n",
    "big_mart_sales.Outlet_Size.replace({'Small': 0, 'Medium': 1, 'High': 2}, inplace=True)\n",
    "\n",
    "# Setting the Grocery Store category Outlet_Type feature column to 0\n",
    "big_mart_sales.Outlet_Type.replace({'Grocery Store': 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e46cefde-13fc-4ea3-b7ea-5451ea9c413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet_identifier_filler = {\n",
    "    v: i \n",
    "    for i, v in enumerate(sorted([\n",
    "        int(val[4:]) for val in big_mart_sales.Outlet_Identifier.unique()\n",
    "    ]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79ee178a-6c25-461e-b0b0-b5e6642dada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the Outlet_Identifier feature column\n",
    "big_mart_sales.Outlet_Identifier = big_mart_sales.Outlet_Identifier.map(\n",
    "    lambda x: outlet_identifier_filler[int(x[4:])]\n",
    ")\n",
    "\n",
    "# Filling the Outlet_Location_Type feature column\n",
    "big_mart_sales.Outlet_Location_Type = big_mart_sales.Outlet_Location_Type.map(\n",
    "    lambda x: int(x[-1]) - 1\n",
    ")\n",
    "\n",
    "# Filling the Outlet_Type feature column\n",
    "big_mart_sales.Outlet_Type = big_mart_sales.Outlet_Type.map(\n",
    "    lambda x: int(x[-1]) if type(x) == str else x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52d8bc-9a81-4ca0-94ca-a4d7b15398fc",
   "metadata": {},
   "source": [
    "> **‚ö†Ô∏è**: I'm just realizing this. The codes in cells 13 and 14 might be hard to interpret based on your level of familiarity with python programming. You could address this cells by decomposing the variable `outlet_identifier_filler` to know what we were really trying to attempt there. You can start by printing this out - `print([int(val[4:]) for val in big_mart_sales.Outlet_Identifier.unique()])`\n",
    "\n",
    "Next would be applying the label encoder to the `Item_Identifier` and `Item_Type` feature column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a8d9826-2aa4-4643-9f3b-9be4934ca660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5fb3603-413c-4ba0-8d51-671aa8727779",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_mart_sales.loc[:, ['Item_Identifier', 'Item_Type']] = big_mart_sales[\n",
    "    ['Item_Identifier', 'Item_Type']\n",
    "].apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78d22fe1-c067-4ea0-8195-39ccf81095f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Identifier</th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Fat_Content</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_Type</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Identifier</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Outlet_Size</th>\n",
       "      <th>Outlet_Location_Type</th>\n",
       "      <th>Outlet_Type</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016047</td>\n",
       "      <td>4</td>\n",
       "      <td>249.8092</td>\n",
       "      <td>9</td>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3735.1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>5.92</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019278</td>\n",
       "      <td>14</td>\n",
       "      <td>48.2692</td>\n",
       "      <td>3</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>443.4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>662</td>\n",
       "      <td>17.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>10</td>\n",
       "      <td>141.6180</td>\n",
       "      <td>9</td>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2097.2700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1121</td>\n",
       "      <td>19.20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>182.0950</td>\n",
       "      <td>0</td>\n",
       "      <td>1998</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>732.3800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1297</td>\n",
       "      <td>8.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>53.8614</td>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>994.7052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Item_Identifier  Item_Weight  Item_Fat_Content  Item_Visibility  Item_Type  \\\n",
       "0              156         9.30                 0         0.016047          4   \n",
       "1                8         5.92                 1         0.019278         14   \n",
       "2              662        17.50                 0         0.016760         10   \n",
       "3             1121        19.20                 1         0.000000          6   \n",
       "4             1297         8.93                 0         0.000000          9   \n",
       "\n",
       "   Item_MRP  Outlet_Identifier  Outlet_Establishment_Year  Outlet_Size  \\\n",
       "0  249.8092                  9                       1999            1   \n",
       "1   48.2692                  3                       2009            1   \n",
       "2  141.6180                  9                       1999            1   \n",
       "3  182.0950                  0                       1998            1   \n",
       "4   53.8614                  1                       1987            2   \n",
       "\n",
       "   Outlet_Location_Type  Outlet_Type  Item_Outlet_Sales  \n",
       "0                     0            1          3735.1380  \n",
       "1                     2            2           443.4228  \n",
       "2                     0            1          2097.2700  \n",
       "3                     2            0           732.3800  \n",
       "4                     2            1           994.7052  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_mart_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039f6a74-9e2c-4da0-8d5d-3837141d750d",
   "metadata": {},
   "source": [
    "So that's it for **feature engineering**. ‚úî\n",
    "\n",
    "#### Feature Scaling\n",
    "---\n",
    "\n",
    "Next is the feature scaling. **Feature scaling** is a method used to normalize the range of independent variables or features of data. We can apply feature scaling to the `Item_MRP` feature column.\n",
    "\n",
    "There are two kinds of scalar that scikit learn provides: min-max scaling and standardization\n",
    "\n",
    "Min-max scaling (often called normalization) is the simplest: values are shifted and re-scaled so that they end up ranging from 0 to 1. this is done by subtracting the min value and dividing by the max minus the min. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a `feature_range` hyperparameter that lets you change the range if, for some reason, you don‚Äôt want 0‚Äì1.\n",
    "\n",
    "Standardization is different: first, it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has a unit variance. \n",
    "\n",
    "Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However, standardization is much less affected by outliers.\n",
    "\n",
    "We would be applying Standardization to the `Item_MRP` feature column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7cb3909-83dc-4a1f-8716-74c79c3ddd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(rc={'figure.figsize':(12,5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "446ba8f5-72ae-4f85-9764-843652f83d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x2ed271b8cd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(big_mart_sales, x=\"Item_MRP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08732a1e-de42-458a-b34b-ae39927ee65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4295dfb5-e568-42f6-ad21-378d14af7afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_mrp_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9759c8d-c783-4dfe-b512-6a450353bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_mrp = big_mart_sales.Item_MRP.to_numpy().reshape(-1, 1)\n",
    "\n",
    "big_mart_sales.Item_MRP = item_mrp_scaler.fit_transform(item_mrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8972a18-6d35-4175-a9d5-a0516500a511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x2ed2c65a490>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(big_mart_sales, x=\"Item_MRP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f67bd26-bd27-4d40-ba4b-978da12620d6",
   "metadata": {},
   "source": [
    "You would notice that there is no difference between the scaled Item_MRP and the original Item_MRP, but the values are considerably smaller.\n",
    "\n",
    "So that concludes the feature scaling. ‚úî\n",
    "\n",
    "> **üõ†** Creating a pure function to handle the data cleaning and feature engineering process is essential to creating a pure data modeling pipeline. Create a function that prepares and engineer the features as we did above.\n",
    "\n",
    "> <h3 style='display: inline;'>üÜï</h3>, üõ†: Create feature scaling functions that scales both vector and matrix alike from scratch. Perform this task for both the min-max scalar ($\\frac{x - x_{min}}{x_{max} - x_{min}}$) and standard scalars ($\\frac{x - \\mu}{\\sigma}$). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4fc822-f556-4874-aa54-7d352e321f2d",
   "metadata": {},
   "source": [
    "Next on our checklist is Shortlisting a model; But before we proceed to that, we'll the addressing the data and it's target. The `Item_Outlet_Sales` column is what we hope to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2d1acaf-58ca-459e-8d24-c0ecdc1b67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'Item_Outlet_Sales'\n",
    "\n",
    "target = big_mart_sales[target_column]\n",
    "data = big_mart_sales.drop(target_column, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b609ab6-6ca2-4083-8e6f-a8257c163e0a",
   "metadata": {},
   "source": [
    "Now that we've successfully addressed our dataset (by splitting it into the data and target). We then proceed to split the data into the train, dev (cross-validation), and test set.\n",
    "\n",
    "In order to split the dataset, we would be making use of the SciKit-Learn `train_test_split` function.\n",
    "\n",
    "The dataset has 8,523 rows, having about 2,000 rows for testing, 1,000 rows for validating, and 5,000 rows for training is good enough. So let's proceed to split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a84a9b51-a6f8-41e6-9d6a-d8460455e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26a8bee8-e71d-4662-9fa1-621e4e6fcfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2660c2fd-097b-4718-9c4c-a30065a1308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ac6548d-58ad-482f-a04c-f7909aee9db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4794,), (2131,), (1598,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2892a06b-f44e-49cf-97fe-6c1e0fdf82a0",
   "metadata": {},
   "source": [
    "#### Shortlisting Promising Models\n",
    "---\n",
    "\n",
    "We have 4,794 training data, 2,131 testing data and 1,598 validation data. With that set in place, we can thus proceed to shortlisting models for our experiment. We would be electing six (6) models which would be:\n",
    "\n",
    "1. Linear Regression\n",
    "2. Random Forest Regressor\n",
    "3. KNN Regressor\n",
    "4. Support Vector Regressor\n",
    "5. XGBoost Regressor\n",
    "6. Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e70ba306-9742-42b9-b327-a81ca14dc6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484d69e0-9276-480f-8201-6df674b9f9a8",
   "metadata": {},
   "source": [
    "> **üìñ**: Now that we've shortlisted these models, we'll be performing a series of actions to check for the model performance before we select a model for our task. The process is called Model Selection. I wrote an article on the [Steps Involved in Selecting a Model](https://gmolalekan.medium.com/steps-involved-in-selecting-a-model-model-selection-bd7aaffbec4f) which should better explain most of what I did below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aab1db25-52fd-4651-95b8-46948db92c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, pickle\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e3d1ff2-dff0-43e6-b361-e7958be5f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_model_evaluation(\n",
    "    verbose=True, time_tracker=True, peep_on_performance=True, \n",
    "    save_progress=True, progress_on_record=None, **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs evaluation on the various imputed models\n",
    "    \n",
    "    This is a pretty large function tarloyed to this\n",
    "    task. \n",
    "    \n",
    "    It has a bunch of parameters to help monitor\n",
    "    the activities going on within the function\n",
    "    \n",
    "    verbose: Gives us details about the steps we are \n",
    "            on and the progress we're making. If True\n",
    "    time_tracker: Tracks the time each model evaluation\n",
    "            takes. If True\n",
    "    peep_on_performance: Presents model performance at\n",
    "            certain stages. If True\n",
    "    save_progress: Saves the progress on evaluation to\n",
    "            data/record.p at the end of each model evaluation.\n",
    "            If True\n",
    "    progress_on_record: Takes an already made progress\n",
    "            and proceeds from that. If provided.\n",
    "            File location in str needs to be provided.\n",
    "    **kwargs: Other variables which would be essential\n",
    "            to the operation of the function. They\n",
    "            include: model_names, X_train, X_val,\n",
    "                y_train, y_val, models, param_grid\n",
    "    \"\"\"\n",
    "    \n",
    "    # Variables downloading\n",
    "    model_names = kwargs['model_names']\n",
    "    X_train = kwargs['X_train']\n",
    "    X_val = kwargs['X_val']\n",
    "    y_train = kwargs['y_train']\n",
    "    y_val = kwargs['y_val']\n",
    "    models = kwargs['models']\n",
    "    param_grid = kwargs['param_grid']\n",
    "    \n",
    "    # Subroutine for time tracking\n",
    "    def cal_time_in_mins(start, _end, process, sep=\",\", end=f\"\\n{'+'*120}\\n\\n\"):\n",
    "        if time_tracker:\n",
    "            total_time_in_sec = round(_end - start)\n",
    "            mins = total_time_in_sec // 60\n",
    "            sec = total_time_in_sec % 60\n",
    "            print(f\"{process} ran for: {mins} mins, {sec} sec\", sep=sep, end=end)\n",
    "            \n",
    "    # Subroutine for verbose messages\n",
    "    def verbose_message(message, sep=\",\", end=f\"\\n{'-'*120}\\n\\n\"):\n",
    "        if verbose:\n",
    "            print(message, sep=sep, end=end)\n",
    "    \n",
    "    # Subroutine to save progress\n",
    "    def preform_progress_saving(r):\n",
    "        if save_progress:\n",
    "            pickle.dump(r, open(\"data/record_r.p\", 'wb'))\n",
    "    \n",
    "    # Wrapper for estimating the time for a process\n",
    "    def time_estimator_wrapper(function):\n",
    "        \n",
    "        def wrapper(*args):\n",
    "            start = time.time()\n",
    "            results = function(*args)\n",
    "            end = time.time()\n",
    "            \n",
    "            cal_time_in_mins(start, end, \"Performance Process\")\n",
    "\n",
    "            return results\n",
    "        \n",
    "        return wrapper\n",
    "    \n",
    "    # Function to perform cross validation\n",
    "    @time_estimator_wrapper\n",
    "    def perform_cross_val(_m, message):\n",
    "        cv_scores = -cross_val_score(\n",
    "            _m(), X_val, y_val,  cv=5, n_jobs=-1,\n",
    "            scoring='neg_mean_absolute_error'\n",
    "        )\n",
    "        \n",
    "        verbose_message(message)\n",
    "        \n",
    "        return cv_scores, cv_scores.mean()\n",
    "    \n",
    "    # Function to perform grid search evaluation\n",
    "    @time_estimator_wrapper\n",
    "    def perform_grid_search(_m, pg, message):\n",
    "        grid_search = GridSearchCV(\n",
    "            _m(), param_grid=pg, n_jobs=-1, cv=5,\n",
    "            scoring='neg_mean_absolute_error'\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        verbose_message(message)\n",
    "        \n",
    "        return grid_search\n",
    "    \n",
    "    # Initializes record\n",
    "    if progress_on_record is None:\n",
    "        record = {name: {} for name in model_names}\n",
    "        verbose_message(\"Initialized a new record\")\n",
    "    else:\n",
    "        record = pickle.load(open(progress_on_record, 'rb'))\n",
    "        verbose_message(\"Initialized existing record\")\n",
    "    \n",
    "    verbose_message(\"Starting the models evaluation\")\n",
    "    \n",
    "    overall_program_start = time.time()\n",
    "    \n",
    "    # Proceeding with model evaluations\n",
    "    for name, pg, model in zip(model_names, param_grid, models):\n",
    "        if not bool(record[name]):\n",
    "            cvs, cvsm = perform_cross_val(\n",
    "                model,\n",
    "                f\"Done performing cross validation for {name}\"\n",
    "            )\n",
    "\n",
    "            gs = perform_grid_search(\n",
    "                model, pg,\n",
    "                f\"Done performing grid search for {name}\"\n",
    "            )\n",
    "\n",
    "            if peep_on_performance:\n",
    "                print(\"Model Performance\", name, cvsm, gs.best_params_, end=f\"\\n{'*'*120}\\n\\n\")\n",
    "\n",
    "            record[name]['cross val score'] = cvs\n",
    "            record[name]['cross val score mean'] = cvsm\n",
    "            record[name]['grid search'] = gs\n",
    "\n",
    "            preform_progress_saving(record)\n",
    "    \n",
    "    overall_program_end = time.time()\n",
    "    \n",
    "    cal_time_in_mins(overall_program_start, overall_program_end, \"Program time\")\n",
    "    \n",
    "    preform_progress_saving(record)\n",
    "    \n",
    "    return record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c44dff-955e-4bf4-8441-cebdfcaa0419",
   "metadata": {},
   "source": [
    "> **üõ†** The function above might better be appreciated as a class but keeping it as a function make it easier for this tutorial. Convert the function above to a class. You don't have to do what I did precisely, the goal is to achieve a similar result using OOP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10a60810-592c-4621-aaea-71ce145f9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LinearRegression, KNeighborsRegressor, DecisionTreeRegressor, \n",
    "    SVR, RandomForestRegressor, XGBRegressor\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a302e521-ae46-4ac4-85f3-2e4aecddb763",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'Linear Regression', 'K-Neighbor Regressor', 'Decision Tree Regressor', \n",
    "    'Support Vector Regressor', 'Random Forest Regressor', 'XGBoost Regressor'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab370a9d-cb77-49a4-ab4d-8f6e5dfa689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We would just try out something simple\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'fit_intercept': [True, False]\n",
    "    },\n",
    "    {\n",
    "        'n_neighbors': range(3, 8),\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'leaf_size': range(10, 51, 10),\n",
    "    },\n",
    "    {\n",
    "        'min_samples_split': range(2, 11)\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['poly', 'rbf'],\n",
    "        'C': range(1, 11)\n",
    "    },\n",
    "    {\n",
    "        'n_estimators': range(50, 501, 50),\n",
    "        'min_samples_split': range(2, 11)\n",
    "    },\n",
    "    {\n",
    "        'n_estimators': range(50, 501, 50),\n",
    "        'min_samples_split': range(2, 11),\n",
    "        \n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa41ae47-72a3-4af1-93d4-c0831594ad7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized a new record\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Starting the models evaluation\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Done performing cross validation for Linear Regression\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Performance Process ran for: 0 mins, 5 sec\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Done performing grid search for Linear Regression\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Performance Process ran for: 0 mins, 0 sec\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Model Performance Linear Regression 912.4731284752081 {'fit_intercept': False}\n",
      "************************************************************************************************************************\n",
      "\n",
      "Done performing cross validation for K-Neighbor Regressor\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Performance Process ran for: 0 mins, 0 sec\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Done performing grid search for K-Neighbor Regressor\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Performance Process ran for: 0 mins, 3 sec\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Model Performance K-Neighbor Regressor 1421.5358163068965 {'leaf_size': 10, 'n_neighbors': 6, 'weights': 'distance'}\n",
      "************************************************************************************************************************\n",
      "\n",
      "Done performing cross validation for Decision Tree Regressor\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Performance Process ran for: 0 mins, 0 sec\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Done performing grid search for Decision Tree Regressor\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Performance Process ran for: 0 mins, 1 sec\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Model Performance Decision Tree Regressor 1064.0510054674764 {'min_samples_split': 10}\n",
      "************************************************************************************************************************\n",
      "\n",
      "Done performing cross validation for Support Vector Regressor\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Performance Process ran for: 0 mins, 1 sec\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Done performing grid search for Support Vector Regressor\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Performance Process ran for: 1 mins, 13 sec\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Model Performance Support Vector Regressor 1304.7126649228896 {'C': 1, 'kernel': 'rbf'}\n",
      "************************************************************************************************************************\n",
      "\n",
      "Done performing cross validation for Random Forest Regressor\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Performance Process ran for: 0 mins, 2 sec\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Done performing grid search for Random Forest Regressor\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Performance Process ran for: 15 mins, 22 sec\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Model Performance Random Forest Regressor 784.1375843873628 {'min_samples_split': 10, 'n_estimators': 300}\n",
      "************************************************************************************************************************\n",
      "\n",
      "Done performing cross validation for XGBoost Regressor\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Performance Process ran for: 0 mins, 1 sec\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "[01:34:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "Done performing grid search for XGBoost Regressor\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Performance Process ran for: 7 mins, 25 sec\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Model Performance XGBoost Regressor 864.508233245642 {'min_samples_split': 2, 'n_estimators': 50}\n",
      "************************************************************************************************************************\n",
      "\n",
      "Program time ran for: 24 mins, 13 sec\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "record = perform_model_evaluation(\n",
    "    X_train=X_train, X_val=X_val, y_train=y_train, y_val=y_val,\n",
    "    models=models, model_names=model_names, param_grid=param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9835ef6-0aaa-444a-aa81-50d7338c2860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the code below if you already \n",
    "# started the program but stopped for a reason.\n",
    "\n",
    "# record = perform_model_evaluation(\n",
    "#     X_train=X_train, X_val=X_val, y_train=y_train, y_val=y_val,\n",
    "#     models=models, model_names=model_names, param_grid=param_grid,\n",
    "#     progress_on_record=\"data/record_r.p\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1272b05-af1a-4c3d-adf3-a08fdd10d057",
   "metadata": {},
   "source": [
    "Just from our little analysis, we see that the model with the best mean cross-validation score is the Random Forest Regressor with a score of approximately 785.54. This is good but it doesn't explain the bulk of what we did.\n",
    "\n",
    "Luckily for us, we have all the information on the performance of each model stored in the record dictionary. thus instead of just peeking into the model performance, we can further evaluate it.\n",
    "\n",
    "> **Special Note**: If you ran this notebook once, the data record has been saved to data/record.p. You can load it with pickle using `pickle.load(open('data/record.p', 'rb'))`\n",
    "\n",
    "Let's start with the linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0a1612e-abec-4348-9960-18c94e336688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cross val score': array([1017.08526401,  867.87766099,  964.15238846,  897.73585079,\n",
       "         815.51447812]),\n",
       " 'cross val score mean': 912.4731284752081,\n",
       " 'grid search': GridSearchCV(cv=5, estimator=LinearRegression(), n_jobs=-1,\n",
       "              param_grid={'fit_intercept': [True, False]},\n",
       "              scoring='neg_mean_absolute_error')}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record['Linear Regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "042a766f-96ae-4136-aca1-119885b5c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_grid_search = record['Linear Regression']['grid search']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10a14cb7-cd14-48f6-9764-286daf8a558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_best_estimator = linear_regression_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b757483b-39fd-4356-a4d0-7c5d0ae3b16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(fit_intercept=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression_best_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b18c13f-b8e9-466c-8c09-e549c193a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_on_val_set = linear_regression_best_estimator.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83fe2c9-9112-4fa9-bd50-27841093de27",
   "metadata": {},
   "source": [
    "Now we can make use of the Scikit-Learn metrics module to perform an evaluation on the validation set to judge the effectiveness of the model on unseen data.\n",
    "\n",
    "There are two major metrics often used for regression tasks. mean absolute error and mean squared - error among others.\n",
    "\n",
    "##### **Mean Absolute Error**\n",
    "\n",
    "The mean absolute error of a model with respect to a test set is the mean of the absolute values of the individual prediction errors on all instances in the test set.\n",
    "\n",
    "![MAE](images/mae.jpg)\n",
    "\n",
    "Think of it as calculating the distance of each point in the regression graph to the regression line, and then finding the average of this distance. This metric gives us a sense of how bad or well the regression line is actually fitting the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ea74a8-1377-4e6f-8790-bc075f85fc07",
   "metadata": {},
   "source": [
    "##### **Mean Squared Error**\n",
    "\n",
    "The mean squared error (MSE) tells us how close a regression line is to a set of points. Like the MAE it does this by taking the distances from the points to the regression line. These distances (the ‚Äúerrors‚Äù) and squares them. The squaring is necessary to remove any negative signs.\n",
    "\n",
    "![MSE](images/mse.png)\n",
    "\n",
    "The MSE is preferred over the MAE if there is a need for a greater degree of accuracy between the points and the regression line as the MSE errors are much higher because of the squared error.\n",
    "\n",
    "We'll be making use of the MAE because our concern is only on the absolute error our model generates\n",
    "\n",
    "> **üõ†**: Since the MSE is always much higher than the MAE, it can be considered very useful when trying to improve the model. Improve this model using the MSE metrics on the final evaluation (which is after going through the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "747b9e17-ea30-442b-b5ec-fdeca106fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae845840-8fe7-409a-9fe9-fd5186cf5f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(905.3018719782131, 912.4731284752081)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, predictions_on_val_set), record['Linear Regression']['cross val score mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df4b740-7478-4950-9fa0-69a2b4a25273",
   "metadata": {},
   "source": [
    "Above is the model evaluation on the evaluation on the dev (cross-validation) set after being trained on the training set -VS- the evaluation on the cross-validation applied on the dev (cross-validation) set from our `perform_model_evaluation` function.\n",
    "\n",
    "> **‚Åâ**: Can you figure out why we made use of the dev (cross-validation) set to perform cross-validation evaluation in our `perform_model_evaluation` function?\n",
    "\n",
    "That said, we can tell that the Linear Regression model performs better on new data (i.e. It generalizes better)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b74c12e-434b-4c64-a26d-f21137c25254",
   "metadata": {},
   "source": [
    "Next would be the K-Neighbor Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1d3d2a0-4099-4753-8a17-206c9981074e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cross val score': array([1476.997404  , 1349.15340087, 1449.39874562, 1429.02219361,\n",
       "        1403.10733743]),\n",
       " 'cross val score mean': 1421.5358163068965,\n",
       " 'grid search': GridSearchCV(cv=5, estimator=KNeighborsRegressor(), n_jobs=-1,\n",
       "              param_grid={'leaf_size': range(10, 51, 10),\n",
       "                          'n_neighbors': range(3, 8),\n",
       "                          'weights': ['uniform', 'distance']},\n",
       "              scoring='neg_mean_absolute_error')}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record['K-Neighbor Regressor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce8840e-5d36-4ee9-85dc-41a51b4fde18",
   "metadata": {},
   "source": [
    "We can already tell that this model performs worst than our linear regression model. But let's not conclude immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a4cc427-84fd-49f4-a821-1e2919acb626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(leaf_size=10, n_neighbors=6, weights='distance')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_grid_search = record['K-Neighbor Regressor']['grid search']\n",
    "knn_best_estimator = knn_grid_search.best_estimator_\n",
    "knn_best_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d366500-b86f-4ca3-8772-13238e69d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_on_val_set = knn_best_estimator.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9410f942-6424-454f-b711-03ed31141f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1310.2587426940163, 1421.5358163068965)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, predictions_on_val_set), record['K-Neighbor Regressor']['cross val score mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb32ce4-a233-4343-a88c-a41463153d9d",
   "metadata": {},
   "source": [
    "It definitely performs worst than the Linear Regression model, but it as improves to new instance too like the Linear Regression model.\n",
    "\n",
    "Next is the Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "015eec20-9f56-49b7-8022-615d7d866dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cross val score': array([1146.34115   , 1114.35154063, 1037.14578875, 1016.08385016,\n",
       "        1006.33269781]),\n",
       " 'cross val score mean': 1064.0510054674764,\n",
       " 'grid search': GridSearchCV(cv=5, estimator=DecisionTreeRegressor(), n_jobs=-1,\n",
       "              param_grid={'min_samples_split': range(2, 11)},\n",
       "              scoring='neg_mean_absolute_error')}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record['Decision Tree Regressor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d40f17-f50c-44e5-bb49-2084968575ca",
   "metadata": {},
   "source": [
    "The performance is rather in between the K-Neighbor Regressor and Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e370038f-c2f6-492b-aff9-d661c16aef92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(min_samples_split=10)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree_grid_search = record['Decision Tree Regressor']['grid search']\n",
    "decision_tree_best_estimator = decision_tree_grid_search.best_estimator_\n",
    "decision_tree_best_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e415ba02-0e88-410d-93db-bb3c210d1f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_on_val_set = decision_tree_best_estimator.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c703f30-8ac4-49af-bb61-ce92de732980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(988.6877413439419, 1064.0510054674764)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, predictions_on_val_set), record['Decision Tree Regressor']['cross val score mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15f2a7-1f14-40f8-8417-38eb3c920a05",
   "metadata": {},
   "source": [
    "All the same, and the Linear regression model remains our best model.\n",
    "\n",
    "Next is the Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e0216faa-41d3-40f0-8285-0478bda28995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cross val score': array([1380.33413646, 1261.61198056, 1380.41190643, 1250.96741024,\n",
       "        1250.23789092]),\n",
       " 'cross val score mean': 1304.7126649228896,\n",
       " 'grid search': GridSearchCV(cv=5, estimator=SVR(), n_jobs=-1,\n",
       "              param_grid={'C': range(1, 11), 'kernel': ['poly', 'rbf']},\n",
       "              scoring='neg_mean_absolute_error')}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record['Support Vector Regressor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "67b8c18d-f109-406b-a431-e59bda4b5a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_grid_search = record['Support Vector Regressor']['grid search']\n",
    "svr_best_estimator = svr_grid_search.best_estimator_\n",
    "svr_best_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ed9fe75-bc3e-409f-9813-dc3a8fb7b46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_on_val_set = svr_best_estimator.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fbd69229-e46e-43b4-abdf-2306a7e3690a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1311.1821775457977, 1304.7126649228896)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, predictions_on_val_set), record['Support Vector Regressor']['cross val score mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988929fa-bae2-49ed-a8fb-e5604d40da3b",
   "metadata": {},
   "source": [
    "The Support Vector Regressor failed to generalize.\n",
    "\n",
    "> **‚Åâ**: For once we have a model that actually performs worst to new instances even after applying grid search üíî, what could possible be the course?\n",
    "\n",
    "Next is the Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "57b5b634-04e2-490f-bf6a-edaa5f2c65cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cross val score': array([840.80494743, 800.9544039 , 812.67292083, 801.2587632 ,\n",
       "        664.99688658]),\n",
       " 'cross val score mean': 784.1375843873628,\n",
       " 'grid search': GridSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "              param_grid={'min_samples_split': range(2, 11),\n",
       "                          'n_estimators': range(50, 501, 50)},\n",
       "              scoring='neg_mean_absolute_error')}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record['Random Forest Regressor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c497fa-44d2-4a85-9d17-ebe7f27432bd",
   "metadata": {},
   "source": [
    "This is promising, we might be seeing some change in power üòÖ. But again, let's not jump into conclusion yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aa23b9ed-2366-4229-a5c7-c63f88463b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(min_samples_split=10, n_estimators=300)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_grid_search = record['Random Forest Regressor']['grid search']\n",
    "random_forest_best_estimator = random_forest_grid_search.best_estimator_\n",
    "random_forest_best_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ef457642-257c-4484-b064-00ac370a37f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_on_val_set = random_forest_best_estimator.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "83cc8e82-c5a5-4f43-af89-50d7bc77523e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(775.2661445193055, 784.1375843873628)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, predictions_on_val_set), record['Random Forest Regressor']['cross val score mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86198470-51d0-406d-aa37-21adbf2cbe74",
   "metadata": {},
   "source": [
    "Beautiful! üòç. We finally get a model that surpasses the Linear Regression model with a mean absolute error of 775.75. That's much better than the mean absolute error of 905.30 by the Linear Regression model.\n",
    "\n",
    "So we have a new champion!!! üèãÔ∏è‚Äç‚ôÄÔ∏è. Anyways the final model is the xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "08538b8f-c2c0-45eb-9f42-bc0bead988b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cross val score': array([898.40030014, 886.35848264, 888.8996433 , 884.23335129,\n",
       "        764.64938887]),\n",
       " 'cross val score mean': 864.508233245642,\n",
       " 'grid search': GridSearchCV(cv=5,\n",
       "              estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     enable_categorical=False, gamma=None,\n",
       "                                     gpu_id=None, importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=None, reg_alpha=None,\n",
       "                                     reg_lambda=None, scale_pos_weight=None,\n",
       "                                     subsample=None, tree_method=None,\n",
       "                                     validate_parameters=None, verbosity=None),\n",
       "              n_jobs=-1,\n",
       "              param_grid={'min_samples_split': range(2, 11),\n",
       "                          'n_estimators': range(50, 501, 50)},\n",
       "              scoring='neg_mean_absolute_error')}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record['XGBoost Regressor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1d1505-16e3-47aa-b09c-d322688e40e9",
   "metadata": {},
   "source": [
    "The XGBoost Regressor is definitely in league with the Random Forest Regressor, let's see if it improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "554788ea-e71e-4c11-ba2e-72d17d6c99aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:34:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.300000012,\n",
       "             max_delta_step=0, max_depth=6, min_child_weight=1,\n",
       "             min_samples_split=2, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=50, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
       "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             subsample=1, tree_method='exact', validate_parameters=1,\n",
       "             verbosity=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_regressor_grid_search = record['XGBoost Regressor']['grid search']\n",
    "xgboost_regressor_best_estimator = xgboost_regressor_grid_search.best_estimator_\n",
    "xgboost_regressor_best_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "572a4894-e7ea-4033-ae85-27b915a7b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_on_val_set = xgboost_regressor_best_estimator.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5fe94000-f451-4154-9a85-634e589024e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(802.2073165642444, 864.508233245642)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, predictions_on_val_set), record['XGBoost Regressor']['cross val score mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b251aa-aa05-4202-8b0f-2fcb70a82965",
   "metadata": {},
   "source": [
    "> **üõ†**: At the end the Random Forest Regressor proves to be a worthy model, but the XGBoost Regressor (which came second) should by no means be underestimated. For tutorial sake, I would proceed to fine-tuning the Random Forest Regressor and leave the fine tuning of the XGBoost Regressor as an assignment. Let's see what you guys can achieve.\n",
    "\n",
    "Well! Now that we've successfully chosen our model ‚úî, let's proceed to fine-tuning the model, to perfect it for the final process (Presenting it as our solution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d393848f-66f8-456a-9ec3-3c76cdd35b63",
   "metadata": {},
   "source": [
    "#### Fine-Tuning The Model\n",
    "---\n",
    "\n",
    "Before fine-tuning a model it is essential to know a lot about the model hyper-parameters. Knowing that would be extremely essential to determining the values of hyper-parameter by hand, or those that should be filtered to get the best result using a grid search.\n",
    "\n",
    "Since we picked the random forest regressor, we would be looking at the hyper-parameters of the random forest regressor.\n",
    "\n",
    "Random Forest is nothing but a bagged version of decision trees. What this means (implies), is that random forest is an ensemble (collection) of decision tree models merged together to make predictions on data. Random forest achieves a lower test error solely by variance reduction. Therefore increasing the number of trees in the ensemble won't have any effect on the bias of the model. The higher number of trees will only reduce the variance of your model.\n",
    "\n",
    "Let's look into the various hyper-parameter considerations, tips and tricks of the random forest models. \n",
    "\n",
    "> **üìñ**: I would be taking my notes directly from [z_ai](https://z-ai.medium.com/) in their article [Random Forest: Hyperparameters and how to fine-tune them](https://towardsdatascience.com/random-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d).\n",
    "\n",
    "- The **N¬∫ of Decision Trees** in the forest (in Scikit-learn this parameter is called n_estimators)\n",
    "- The **criteria** with which to split on each node (Gini or Entropy for a classification task, or the MSE or MAE for regression)\n",
    "- The **maximum depth** of the individual trees. The larger an individual tree, the more chance it has of overfitting the training data, however, as in Random Forests we have many individual trees, this is not such a big problem.\n",
    "- The **minimum samples** to split on at an internal node of the trees. Playing with this parameter and the previous one we could regularise the individual trees if needed.\n",
    "- **The maximum number of leaf nodes**. In Random Forest this is not so important, but in an individual Decision Tree, it can greatly help reduce over-fitting as well and also help increase the explainability of the tree by reducing the possible number of paths to leaf nodes. Learn how to use Decision Trees to build explainable ML models here.\n",
    "- **The number of random features** to include at each node for splitting.\n",
    "- **The size of the bootstrapped dataset** to train each Decision Tree with.\n",
    "\n",
    "Fine-tuning the **number of trees** (N¬∫ of Decision Trees) is unnecessary because the secret to selecting a precise number of estimators is by evaluating the dataset to estimate the number of trees that would create a robust model with less variance and low cost of training time. Given our data size, I would an estimators of the value `500` is good enough, knowing that the larger the number of estimator the lower the variance and the more computational power required.\n",
    "\n",
    "For the criterion, the general rule of thumb for the regression task is:\n",
    "\n",
    "> Take MSE if you don‚Äôt have many outliers in your data, as it penalises highly those observations that are far away from the mean.\n",
    "\n",
    "Since we don't have a great mass of outliers from our EDA (Exploratory Data Analysis) classes using the **MSE** criterion is probably a safe bet for us.\n",
    "\n",
    "Increasing the depth of individual trees increases the possible number of feature/value combinations that are taken into account. The deeper the tree, the more splits it has and the more information about the data it takes into account. It may not be prone to over-fitting because it's an ensemble model but it is still possible to have large depth values. Picking values within the **range of 6 to 9** should be adequate for the model.\n",
    "\n",
    "The number of random features to consider at each split is one of the most important hyper-parameters to tune in a Random Forest ensemble. Since we're working on a regression task we would be using the values `[None, .2, .3, .4]` to allow the random forest to take all the features of your data, 20%, 30%, and 40% of variables in individual split respectively.\n",
    "\n",
    "Let's try out these values in a grid search and measure how much our model would evolve on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33830a6f-9762-41fe-b50d-400ea609cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': range(6, 10),\n",
    "    'max_features': [None, .2, .3, .4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df5505f0-7150-4e23-bbcd-da29913d602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    RandomForestRegressor(n_estimators=500, criterion='mse'),\n",
    "    param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error', verbose=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "97ce4787-5410-401d-8ec3-577357c54f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestRegressor(n_estimators=500), n_jobs=-1,\n",
       "             param_grid={'max_depth': range(6, 10),\n",
       "                         'max_features': [None, 0.2, 0.3, 0.4]},\n",
       "             scoring='neg_mean_absolute_error', verbose=3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "263d36b4-6fd8-414d-b795-a75abd5ac8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 6, 'max_features': None}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc6bf00-2e5c-4a22-985d-930fe568295e",
   "metadata": {},
   "source": [
    "With grid search we've been able to decide on this parameters. Now let's evaluate our new model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1e5d99e3-27f7-4b77-a6eb-96498121538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_mart_predictor = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "16b93a4a-b396-4354-97a8-a9b3ebad77f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=6, max_features=None, n_estimators=500)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_mart_predictor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c4c72bf4-e558-46e2-a594-2fa8620b5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = big_mart_predictor.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fe2027c9-c428-4e74-909d-fc15c0f270dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "753.405502776726"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7800914a-da2d-4680-b3d6-61344041b514",
   "metadata": {},
   "source": [
    "We can see there's a slight improvement. It is definitely better than our first set of evaluations. We're done fine-tuning the model. ‚úî\n",
    "\n",
    "Now for the most important part; to see the model performance on data it has never seen before, which is the test set. We'll have to merge both the training and dev set. After merging them, we would train the model on the merged set and evaluate the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f5e13991-ae7c-41f6-bece-6e51bb628a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat((X_train, X_val), axis=0)\n",
    "y_train = pd.concat((y_train, y_val), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9cb55862-f728-451f-9821-f8c03d4bdb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "74f5f743-e2b1-404e-823a-34fcdc57dd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=6, max_features=None, n_estimators=500)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6e5144f9-8c4a-4a93-a5b7-f6e2c9b209dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1b0f86f4-33eb-41cd-a9bf-6839b8f4789f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728.8538462557138"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94508f2b-7386-4438-b14a-4285b35895c1",
   "metadata": {},
   "source": [
    "> **‚Åâ**: We see that our model improves further when coming across never-seen data. Could you guess the reason why this is?\n",
    "\n",
    "That concludes the model fine-tuning. ‚úî"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54084156-727b-4de2-b54a-a42ce9baee33",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "So that concludes our session on data modeling for regression tasks. I hope you guys have a deeper intuition on what data modeling in machine learning is all about.\n",
    "\n",
    "There are extra activities for you guys alongside those stated in the black quotes üõ† üëΩ. This may be more complex based on how proficient you're with the python machine learning tools, so it isn't compulsory everyone submits this as part of their assignments.\n",
    "\n",
    "1. Make use of `sklearn.pipeline.Pipeline` and `sklearn.compose.ColumnTransformer` functionalities to build a pipeline for the system.\n",
    "2. What are voting regressors and build a voting regressor that uses both Linear regression and Random Forest regressor to make predictions.\n",
    "3. Make use of the XGBoost to surpass our final score (728.85) document your reasons for success.\n",
    "\n",
    "__Notebook by Ganiyu Olalekan on data modeling in machine learning.__\n",
    "\n",
    "Follow **Olalekan @**:\n",
    "\n",
    "- [LinkedIn](https://www.linkedin.com/in/olalekan-ganiyu-747855199/)\n",
    "- [Twitter](https://twitter.com/GM_Olalekan)\n",
    "- [Medium](https://gmolalekan.medium.com/)\n",
    "- [Kaggle](https://www.kaggle.com/ganiyuolalekan)\n",
    "- [Github](https://github.com/ganiyuolalekan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e697ed77-54c3-4b71-956a-ed0a3eef49a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
